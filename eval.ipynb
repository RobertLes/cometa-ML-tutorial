{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d72549b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "if not(os.path.isfile(\"train_nominal_000.h5\")):\n",
    "    if sys.platform==\"darwin\": #MAC OSX\n",
    "        !curl https://opendata.cern.ch/record/80030/files/assets/atlas/datascience/CERN-EP-2024-159/train_nominal_000.h5.gz --output train_nominal_000.h5.gz\n",
    "        !curl https://opendata.cern.ch/record/80030/files/assets/atlas/datascience/CERN-EP-2024-159/test_nominal_000.h5.gz --output test_nominal_000.h5.gz\n",
    "    elif sys.platform == \"linux\" or sys.platform == \"linux2\":\n",
    "        !wget https://opendata.cern.ch/record/80030/files/assets/atlas/datascience/CERN-EP-2024-159/train_nominal_000.h5.gz\n",
    "        !wget https://opendata.cern.ch/record/80030/files/assets/atlas/datascience/CERN-EP-2024-159/test_nominal_000.h5.gz\n",
    "    !gunzip -f train_nominal_000.h5.gz\n",
    "    !gunzip -f test_nominal_000.h5.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40d377d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from torchinfo import summary\n",
    "\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cc7f4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ATLASH5HighLevelDataset(torch.utils.data.Dataset):\n",
    "    def transform(self,data,transform=True):\n",
    "        if transform:\n",
    "            #Calculate some metrics from subsample of total\n",
    "            vals=[]\n",
    "            Njets=np.max([1000,data.len()])\n",
    "            for jet in range(Njets): vals.append(data[jet])\n",
    "            maxval=np.max(vals)\n",
    "            minval=np.min(vals)\n",
    "            return (data-minval)/(maxval-minval)\n",
    "        else:\n",
    "            return data\n",
    "\n",
    "    def __init__(self, file_path, transform=True, return_pt=False):\n",
    "        super(ATLASH5HighLevelDataset, self).__init__()\n",
    "        h5_file = h5py.File(file_path , 'r')\n",
    "        self.data=torch.tensor([])\n",
    "    \n",
    "        self.C2=self.transform(h5_file['fjet_C2'],transform=transform)\n",
    "        self.D2=self.transform(h5_file['fjet_D2'],transform=transform)\n",
    "        self.ECF1=self.transform(h5_file['fjet_ECF1'],transform=transform)\n",
    "        self.ECF2=self.transform(h5_file['fjet_ECF2'],transform=transform)\n",
    "        self.ECF3=self.transform(h5_file['fjet_ECF3'],transform=transform)\n",
    "        self.L2=self.transform(h5_file['fjet_L2'],transform=transform)\n",
    "        self.L3=self.transform(h5_file['fjet_L3'],transform=transform)\n",
    "        self.Qw=self.transform(h5_file['fjet_Qw'],transform=transform)\n",
    "        self.Split12=self.transform(h5_file['fjet_Split12'],transform=transform)\n",
    "        self.Split23=self.transform(h5_file['fjet_Split23'],transform=transform)\n",
    "        self.Tau1_wta=self.transform(h5_file['fjet_Tau1_wta'],transform=transform)\n",
    "        self.Tau2_wta=self.transform(h5_file['fjet_Tau2_wta'],transform=transform)\n",
    "        self.Tau3_wta=self.transform(h5_file['fjet_Tau3_wta'],transform=transform)\n",
    "        self.Tau4_wta=self.transform(h5_file['fjet_Tau4_wta'],transform=transform)\n",
    "        self.ThrustMaj=self.transform(h5_file['fjet_ThrustMaj'],transform=transform)\n",
    "        self.m=self.transform(h5_file['fjet_m'],transform=transform)\n",
    "    \n",
    "        self.pt = h5_file['fjet_pt']\n",
    "        self.labels = h5_file['labels']\n",
    "        if \"training_weights\" in h5_file:\n",
    "            self.hasWeights=True\n",
    "            self.weights = h5_file['training_weights']\n",
    "        else:\n",
    "            self.hasWeights=False\n",
    "    \n",
    "        self.return_pt = return_pt\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        self.data=torch.tensor([self.D2[index],self.C2[index],self.ECF1[index],self.ECF2[index],self.ECF3[index],self.L2[index],self.L3[index],self.Qw[index],self.Split12[index],self.Split23[index],self.Tau1_wta[index],self.Tau2_wta[index],self.Tau3_wta[index],self.Tau4_wta[index],self.ThrustMaj[index],self.m[index]])\n",
    "    \n",
    "        if self.return_pt:\n",
    "            self.data=self.data,torch.tensor([self.pt[index]])\n",
    "    \n",
    "        if self.hasWeights:\n",
    "            return self.data,torch.tensor(self.labels[index],dtype=torch.int64),torch.tensor(self.weights[index])\n",
    "        else:\n",
    "            return self.data,torch.tensor(self.labels[index],dtype=torch.int64),torch.tensor(1)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b7a14c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ATLAS_inputs(directory,network,batch_size=2**8,return_pt=False,transform=True):\n",
    "    \n",
    "    #Get list of input files\n",
    "    training_list=[]\n",
    "    testing_list=[]\n",
    "    for file in sorted(os.listdir(directory)):\n",
    "        if not \".h5\" in file: continue\n",
    "        if \"train\" in file:\n",
    "            print(\"Using file %s for training\"%(directory+\"/\"+file))\n",
    "            training_list.append(directory+\"/\"+file)\n",
    "        elif \"test\" in file:\n",
    "            print(\"Using file %s for testing\"%(directory+\"/\"+file))\n",
    "            testing_list.append(directory+\"/\"+file)\n",
    "    \n",
    "    #concat them\n",
    "    training_DSlist=[]\n",
    "    testing_DSlist=[]\n",
    "    if network!=\"HL\":\n",
    "        for file in training_list: training_DSlist.append(ATLASH5LowLevelDataset(file,return_pt=return_pt,transform=transform))\n",
    "        for file in testing_list: testing_DSlist.append(ATLASH5LowLevelDataset(file,return_pt=return_pt,transform=transform))\n",
    "    else:\n",
    "        for file in training_list: training_DSlist.append(ATLASH5HighLevelDataset(file,return_pt=return_pt,transform=transform))\n",
    "        for file in testing_list: testing_DSlist.append(ATLASH5HighLevelDataset(file,return_pt=return_pt,transform=transform))\n",
    "    training_data = ConcatDataset(training_DSlist)\n",
    "    testing_data = ConcatDataset(testing_DSlist)\n",
    "    \n",
    "    #load them into torch dataloaders \n",
    "    train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=0)\n",
    "    test_dataloader = DataLoader(testing_data, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    #return\n",
    "    return train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1600d606",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNNetwork(nn.Module):\n",
    "    def __init__(self,Ninputs,useConstituents=False):\n",
    "        super().__init__()\n",
    "        self.useConstituents=useConstituents\n",
    "    \n",
    "        if self.useConstituents: self.flatten = nn.Flatten()\n",
    "        #self.norm=nn.BatchNorm1d(Ninputs)\n",
    "        self.fc1= nn.Linear(Ninputs, 512)\n",
    "        self.act1=nn.ReLU()\n",
    "        self.fc2= nn.Linear(512, 512)\n",
    "        self.act2=nn.ReLU()\n",
    "        self.fc3= nn.Linear(512, 512)\n",
    "        self.act3=nn.ReLU()\n",
    "        self.fc4= nn.Linear(512, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.useConstituents: x = self.flatten(x)\n",
    "        #x=self.norm(x)\n",
    "        f1=self.act1(self.fc1(x))\n",
    "        f2=self.act2(self.fc2(f1))\n",
    "        f3=self.act3(self.fc3(f2))\n",
    "        logits = self.fc4(f3)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be979bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using file .//test_nominal_000.h5 for testing\n",
      "Using file .//train_nominal_000.h5 for training\n"
     ]
    }
   ],
   "source": [
    "train_dataloader,test_dataloader=get_ATLAS_inputs(\"./\",\"HL\",batch_size=2**8,return_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96338556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ndim = len(train_dataloader.dataset[0][0][0])\n",
    "device = \"cpu\"\n",
    "model = DNNetwork(Ndim).to(device)\n",
    "model.load_state_dict(torch.load(\"model.pth\"))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c7b35b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((tensor([6.3456e-02, 1.8466e-01, 2.7753e-01, 1.8973e-02, 2.9963e-04, 7.8222e-01,\n",
      "        5.9016e-01, 6.0296e-02, 2.3117e-02, 5.0525e-02, 9.4161e-02, 1.1921e-01,\n",
      "        7.9511e-02, 8.6709e-02, 2.8462e-01, 4.4461e-02]), tensor([1438239.7500])), tensor(1), tensor(1.))\n"
     ]
    }
   ],
   "source": [
    "print(train_dataloader.dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76da9f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load information into numpy arrays\n",
    "ys=[]\n",
    "preds_raw=[]\n",
    "weights=[]\n",
    "pts=[]\n",
    "with torch.no_grad():\n",
    "    for (X, pt), y, w in test_dataloader:\n",
    "        #Get prediction\n",
    "        pred=model(X)\n",
    "        pred=torch.sigmoid(pred)\n",
    "\n",
    "        #append info\n",
    "        preds_raw.append(pred.numpy())\n",
    "        ys.append(y.numpy())\n",
    "        pts.append(pt.numpy())\n",
    "        weights.append(w.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4492adfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/les/venv/CometaTutorial/lib/python3.7/site-packages/ipykernel_launcher.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  \n",
      "/Users/les/venv/CometaTutorial/lib/python3.7/site-packages/ipykernel_launcher.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/les/venv/CometaTutorial/lib/python3.7/site-packages/ipykernel_launcher.py:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  \"\"\"\n",
      "/Users/les/venv/CometaTutorial/lib/python3.7/site-packages/ipykernel_launcher.py:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#Store things in useful per-jet format\n",
    "preds_raw=np.array(preds_raw)\n",
    "preds_raw=preds_raw.reshape(-1,preds_raw.shape[-1])\n",
    "ys=np.array(ys).flatten()\n",
    "pts=np.array(pts).flatten()\n",
    "weights=np.array(weights).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "130dcc47",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/p_/k_v78m8s1jdfptnnvdw_ml2m0000gn/T/ipykernel_5409/3318279841.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#make confusion matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mconfusion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfusion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "#make confusion matrix\n",
    "confusion=metrics.confusion_matrix(ys, np.argmax(preds_raw,axis=1))\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ef23d4e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "loop of ufunc does not support argument 0 of type numpy.ndarray which has no callable log method",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'log'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/p_/k_v78m8s1jdfptnnvdw_ml2m0000gn/T/ipykernel_5409/3722282888.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Decision rule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpreds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds_raw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds_raw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: loop of ufunc does not support argument 0 of type numpy.ndarray which has no callable log method"
     ]
    }
   ],
   "source": [
    "#Decision rule\n",
    "preds=np.log(preds_raw[:,0])/np.log(preds_raw[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4beb211a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'metrics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/p_/k_v78m8s1jdfptnnvdw_ml2m0000gn/T/ipykernel_5409/1608857035.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#make ROC curve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresholds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtpr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mfpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_yscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'log'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'metrics' is not defined"
     ]
    }
   ],
   "source": [
    "#make ROC curve\n",
    "fpr, tpr, thresholds = metrics.roc_curve(ys, preds, sample_weight=weights)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(tpr,1/fpr)\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlim([0.3, 1.0])\n",
    "fig.savefig(f\"ROC_{outname}.pdf\") \n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1f7521e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'thresholds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/p_/k_v78m8s1jdfptnnvdw_ml2m0000gn/T/ipykernel_5409/3935532561.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Find 50% working point\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mwp50\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthresholds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtpr\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'thresholds' is not defined"
     ]
    }
   ],
   "source": [
    "#Find 50% working point\n",
    "wp50=thresholds[np.argmax(tpr>0.5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8717a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/les/venv/CometaTutorial/lib/python3.7/site-packages/ipykernel_launcher.py:7: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  import sys\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'preds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/p_/k_v78m8s1jdfptnnvdw_ml2m0000gn/T/ipykernel_5409/2565722501.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mbkg_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mys\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mbkg_pts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbkg_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mbkg_preds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbkg_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mii\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbkg_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mbinx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbkg_pts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mii\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'preds' is not defined"
     ]
    }
   ],
   "source": [
    "# make rejection plot at working point\n",
    "Nbins=10\n",
    "bins=np.linspace(0,3e6,Nbins+1,endpoint=True)\n",
    "N_accepted=np.zeros(Nbins)\n",
    "N_all=np.zeros(Nbins)\n",
    "\n",
    "bkg_indices=np.argwhere(ys==0)\n",
    "bkg_pts=pts[bkg_indices]\n",
    "bkg_preds=preds[bkg_indices]\n",
    "for ii in range(len(bkg_indices)):\n",
    "    binx=int(np.floor(bkg_pts[ii]/(bins[1])))\n",
    "    if binx>len(N_all)-1: continue\n",
    "\n",
    "    N_all[binx]+=1\n",
    "    if bkg_preds[ii]>wp50: N_accepted[binx]+=1\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.errorbar((bins[1:]+bins[:-1])/2, np.nan_to_num(N_all/N_accepted), xerr=(bins[1:]-bins[:-1])/2,fmt='o')\n",
    "fig.savefig(f\"rejection_pt_{outname}.pdf\")\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9ff99d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CometaTutorial-venv",
   "language": "python",
   "name": "cometatutorial-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
